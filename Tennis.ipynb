{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/DPRL-Tennis-ML-Agents/python')\n",
    "# sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "np.float_ = np.float64\n",
    "np.int_ = np.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/ubuntu/DPRL-Tennis-ML-Agents/Tennis_Linux/Tennis.x86_64\n",
      "Mono path[0] = '/home/ubuntu/DPRL-Tennis-ML-Agents/Tennis_Linux/Tennis_Data/Managed'\n",
      "Mono config path = '/home/ubuntu/DPRL-Tennis-ML-Agents/Tennis_Linux/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "\tScreenSelector.so\n",
      "Logging to /home/ubuntu/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\", no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 6):                                      # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     while True:\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#         env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __new__(self):\n",
    "        \"\"\"Define this class as a singleton\"\"\"\n",
    "        if not hasattr(self, 'instance'):\n",
    "            self.instance = super().__new__(self)\n",
    "\n",
    "            self.instance.device = None\n",
    "            self.instance.seed = None\n",
    "            self.instance.target_score = None\n",
    "            self.instance.target_episodes = None\n",
    "            self.instance.max_episodes = None\n",
    "\n",
    "            self.instance.state_size = None\n",
    "            self.instance.action_size = None\n",
    "            self.instance.num_agents = None\n",
    "\n",
    "            self.instance.actor_layers = None\n",
    "            self.instance.critic_layers = None\n",
    "            self.instance.actor_lr = None\n",
    "            self.instance.critic_lr = None\n",
    "            self.instance.lr_sched_step = None\n",
    "            self.instance.lr_sched_gamma = None\n",
    "\n",
    "            self.instance.batch_normalization = None\n",
    "\n",
    "            self.instance.buffer_size = None\n",
    "            self.instance.batch_size = None\n",
    "            self.instance.gamma = None\n",
    "            self.instance.tau = None\n",
    "\n",
    "            self.instance.noise = None\n",
    "            self.instance.noise_theta = None\n",
    "            self.instance.noise_sigma = None\n",
    "\n",
    "        return self.instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BaseNN(nn.Module):\n",
    "    \"\"\"Superclass for the Actor and Critic classes\"\"\"\n",
    "    def __init__(self):\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.config = Config()\n",
    "        self.to(self.config.device)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        self.module_list = nn.ModuleList()\n",
    "\n",
    "    def create_fc_layer(self, nodes_in, nodes_out):\n",
    "        layer = nn.Linear(nodes_in, nodes_out)\n",
    "        self.reset_parameters(layer)\n",
    "        self.module_list.append(layer)\n",
    "\n",
    "    def reset_parameters(self, layer):\n",
    "        layer.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "\n",
    "class Actor(BaseNN):\n",
    "    \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        for nodes_in, nodes_out in self.layers_nodes():\n",
    "            self.create_fc_layer(nodes_in, nodes_out)\n",
    "\n",
    "    def layers_nodes(self):\n",
    "        nodes = []\n",
    "        nodes.append(self.config.state_size)\n",
    "        nodes.extend(self.config.actor_layers)\n",
    "        nodes.append(self.config.action_size)\n",
    "        nodes_in = nodes[:-1]\n",
    "        nodes_out = nodes[1:]\n",
    "        return zip(nodes_in, nodes_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.module_list[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.module_list[-1](x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "\n",
    "class Critic(BaseNN):\n",
    "    \"\"\"Build a critic (value) network that maps\n",
    "       (state, action) pair -> Q-values.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        for nodes_in, nodes_out in self.layers_nodes():\n",
    "            self.create_fc_layer(nodes_in, nodes_out)\n",
    "        if self.config.batch_normalization:\n",
    "            self.bn = nn.BatchNorm1d(self.module_list[1].in_features)\n",
    "\n",
    "    def layers_nodes(self):\n",
    "        nodes = []\n",
    "        nodes.append(self.config.state_size * self.config.num_agents)\n",
    "        nodes.extend(self.config.critic_layers)\n",
    "        nodes.append(1)\n",
    "        nodes_in = nodes[:-1]\n",
    "        nodes_in[1] += self.config.num_agents * self.config.action_size\n",
    "        nodes_out = nodes[1:]\n",
    "        return zip(nodes_in, nodes_out)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.module_list[0](state))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        if self.config.batch_normalization:\n",
    "            x = self.bn(x)\n",
    "        for layer in self.module_list[1:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.module_list[-1](x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        random.seed(self.config.seed)\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor_local = Actor()\n",
    "        self.actor_target = Actor()\n",
    "        local_state_dict = self.actor_local.state_dict()\n",
    "        self.actor_target.load_state_dict(local_state_dict)\n",
    "        self.actor_optimizer = optim.Adam(\n",
    "            self.actor_local.parameters(),\n",
    "            lr=self.config.actor_lr)\n",
    "        self.actor_lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.actor_optimizer,\n",
    "            step_size=self.config.lr_sched_step,\n",
    "            gamma=self.config.lr_sched_gamma)\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic_local = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        local_state_dict = self.critic_local.state_dict()\n",
    "        self.critic_target.load_state_dict(local_state_dict)\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            self.critic_local.parameters(),\n",
    "            lr=self.config.critic_lr)\n",
    "        self.critic_lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.critic_optimizer,\n",
    "            step_size=self.config.lr_sched_step,\n",
    "            gamma=self.config.lr_sched_gamma)\n",
    "\n",
    "        # Initialize a noise process\n",
    "        self.noise = OUNoise()\n",
    "\n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update actor and critic parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        tau = self.config.tau\n",
    "        for target_param, local_param \\\n",
    "                in zip(self.actor_target.parameters(),\n",
    "                       self.actor_local.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau * local_param.data\n",
    "                + (1.0 - tau) * target_param.data)\n",
    "        for target_param, local_param \\\n",
    "                in zip(self.critic_target.parameters(),\n",
    "                       self.critic_local.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau * local_param.data\n",
    "                + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.actor_local.eval()\n",
    "            state = torch.from_numpy(state).float()\n",
    "            state.to(self.config.device)\n",
    "            action = self.actor_local(state).data.cpu().numpy()\n",
    "            self.actor_local.train()\n",
    "\n",
    "        if self.config.noise:\n",
    "            action += self.noise.sample()\n",
    "            np.clip(action, a_min=-1, a_max=1, out=action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def lr_step(self):\n",
    "        self.actor_lr_scheduler.step()\n",
    "        self.critic_lr_scheduler.step()\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    def __init__(self, mu=0.):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.config = Config()\n",
    "        random.seed(self.config.seed)\n",
    "        self.mu = mu * np.ones(self.config.action_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        random_array = [random.random() for i in range(len(x))]\n",
    "        dx = self.config.noise_theta * (self.mu - x) \\\n",
    "             + self.config.noise_sigma * np.array(random_array)\n",
    "        self.state = x + dx\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        random.seed(self.config.seed)\n",
    "        self.memory = deque(maxlen=self.config.buffer_size)\n",
    "\n",
    "        self.experience = namedtuple(\n",
    "            'Experience',\n",
    "            field_names=['state', 'actions', 'rewards', 'next_state'])\n",
    "\n",
    "    def store(self, state, actions, reward, next_state):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, actions, reward, next_state)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, self.config.batch_size)\n",
    "\n",
    "        states = self.create_tensor(dim=self.config.state_size)\n",
    "        actions = self.create_tensor(dim=self.config.action_size)\n",
    "        rewards = self.create_tensor()\n",
    "        next_states = self.create_tensor(dim=self.config.state_size)\n",
    "        for i, e in enumerate(experiences):\n",
    "            states[i] = torch.as_tensor(e.state)\n",
    "            actions[i] = torch.as_tensor(e.actions)\n",
    "            rewards[i] = torch.as_tensor(e.rewards)\n",
    "            next_states[i] = torch.as_tensor(e.next_state)\n",
    "        return (states, actions, rewards, next_states)\n",
    "\n",
    "    def create_tensor(self, dim=0):\n",
    "        batch_size = self.config.batch_size\n",
    "        num_agents = self.config.num_agents\n",
    "        if dim > 0:\n",
    "            size = (batch_size, num_agents, dim)\n",
    "        else:\n",
    "            size = (batch_size, num_agents)\n",
    "\n",
    "        tensor = torch.empty(size=size, dtype=torch.float,\n",
    "                             device=self.config.device,\n",
    "                             requires_grad=False)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class MultiAgentDDPG():\n",
    "    \"\"\"Manage multi agents while interacting with the environment.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MultiAgentDDPG, self).__init__()\n",
    "        self.config = Config()\n",
    "        self.agents = [Agent() for _ in range(self.config.num_agents)]\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "    def act(self, state):\n",
    "        actions = [agent.act(obs) \\\n",
    "                   for agent, obs in zip(self.agents, state)]\n",
    "        return actions\n",
    "\n",
    "    def actions_target(self, states):\n",
    "        batch_size = self.config.batch_size\n",
    "        num_agents = self.config.num_agents\n",
    "        action_size = self.config.action_size\n",
    "        with torch.no_grad():\n",
    "            actions = torch.empty(\n",
    "                (batch_size, num_agents, action_size),\n",
    "                device=self.config.device)\n",
    "            for idx, agent in enumerate(self.agents):\n",
    "                actions[:,idx] = agent.actor_target(states[:,idx])\n",
    "        return actions\n",
    "\n",
    "    def actions_local(self, states, agent_id):\n",
    "        batch_size = self.config.batch_size\n",
    "        num_agents = self.config.num_agents\n",
    "        action_size = self.config.action_size\n",
    "\n",
    "        actions = torch.empty(\n",
    "            (batch_size, num_agents, action_size),\n",
    "            device=self.config.device)\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            action = agent.actor_local(states[:,idx])\n",
    "            if not idx == agent_id:\n",
    "                action.detach()\n",
    "            actions[:,idx] = action\n",
    "        return actions\n",
    "\n",
    "    def store(self, state, actions, rewards, next_state):\n",
    "        self.buffer.store(state, actions, rewards, next_state)\n",
    "\n",
    "        if len(self.buffer) >= self.config.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        batch_size = self.config.batch_size\n",
    "        for agent_id, agent in enumerate(self.agents):\n",
    "            # sample a batch of experiences\n",
    "            states, actions, rewards, next_states = self.buffer.sample()\n",
    "            # stack the agents' variables to feed the networks\n",
    "            obs = states.view(batch_size, -1)\n",
    "            actions = actions.view(batch_size, -1)\n",
    "            next_obs = next_states.view(batch_size, -1)\n",
    "            # Consider only the rewards for this agent\n",
    "            r = rewards[:,agent_id].unsqueeze_(1)\n",
    "\n",
    "            ## Train the Critic network\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.actions_target(next_states)\n",
    "                next_actions = next_actions.view(batch_size, -1)\n",
    "                next_q_val = agent.critic_target(next_obs, next_actions)\n",
    "                y = r + self.config.gamma * next_q_val\n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            q_value_predicted = agent.critic_local(obs, actions)\n",
    "            loss = F.mse_loss(q_value_predicted, y)\n",
    "            loss.backward()\n",
    "            agent.critic_optimizer.step()\n",
    "\n",
    "            ## Train the Actor network\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            actions_local = self.actions_local(states, agent_id)\n",
    "            actions_local = actions_local.view(batch_size, -1)\n",
    "            q_value_predicted = agent.critic_local(obs, actions_local)\n",
    "            loss = -q_value_predicted.mean()\n",
    "            loss.backward()\n",
    "            agent.actor_optimizer.step()\n",
    "\n",
    "        for agent in self.agents:\n",
    "            agent.soft_update()\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset_noise()\n",
    "\n",
    "    def state_dict(self):\n",
    "        return [agent.actor_local.state_dict() for agent in self.agents]\n",
    "\n",
    "    def load_state_dict(self, state_dicts):\n",
    "        for agent, state_dict in zip(self.agents, state_dicts):\n",
    "            agent.actor_local.load_state_dict(state_dict)\n",
    "\n",
    "    def lr_step(self):\n",
    "        for agent in self.agents:\n",
    "            agent.lr_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize values in Config\n",
    "config = Config()\n",
    "config.seed = 997\n",
    "config.device = 'cpu'\n",
    "config.target_score = 0.5\n",
    "config.target_episodes = 100\n",
    "\n",
    "config.num_agents = num_agents\n",
    "config.action_size = action_size\n",
    "config.state_size = state_size\n",
    "\n",
    "config.actor_layers = [64, 64]\n",
    "config.critic_layers = [64, 64]\n",
    "config.actor_lr = 3e-3\n",
    "config.critic_lr = 4e-4\n",
    "config.lr_sched_step = 1\n",
    "config.lr_sched_gamma = 0.2\n",
    "config.batch_normalization = True\n",
    "\n",
    "config.buffer_size = int(1e6)\n",
    "config.batch_size = 64\n",
    "config.gamma = 0.99\n",
    "config.tau = 8e-3\n",
    "config.noise = True\n",
    "config.noise_theta = 0.9\n",
    "config.noise_sigma = 0.01\n",
    "\n",
    "config.max_episodes = 2000\n",
    "\n",
    "# Instantiate a Multi Agent\n",
    "maddpg = MultiAgentDDPG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the training function\n",
    "def train(env, maddpg, max_episodes=5000):\n",
    "    \"\"\"Train a Multi Agent Deep Deterministic Policy Gradients (MADDPG).\n",
    "    Params\n",
    "    ======\n",
    "        env (UnityEnvironment): environment for the agents\n",
    "        maddpg (MADDPG): the Multi Agent DDPG\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    agents_scores = []\n",
    "    moving_avg = []\n",
    "    \n",
    "    avg_checkpoints = iter([0.1, 0.2, 0.4, 0.6])\n",
    "    next_avg_checkpoint = next(avg_checkpoints)\n",
    "    ## Perform n_episodes of training\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        maddpg.reset_noise()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "\n",
    "        scores_episode = np.zeros(config.num_agents)\n",
    "        while True:\n",
    "            ## Perform a step: S;A;R;S'\n",
    "            actions = maddpg.act(state)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "\n",
    "            rewards = env_info.rewards\n",
    "            next_state = env_info.vector_observations\n",
    "\n",
    "            maddpg.store(state, actions, rewards, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            scores_episode += rewards\n",
    "\n",
    "            if any(env_info.local_done):\n",
    "                break\n",
    "\n",
    "        agents_scores.append(scores_episode)\n",
    "        scores.append(scores_episode.max())\n",
    "        moving_avg.append(np.mean(scores[-config.target_episodes:]))\n",
    "\n",
    "        if moving_avg[-1] >= next_avg_checkpoint:\n",
    "            print('\\nDecreasing learning rate ...')\n",
    "            maddpg.lr_step()\n",
    "            next_avg_checkpoint = next(avg_checkpoints)\n",
    "\n",
    "        print('\\rEpisode {:4d}\\t' \\\n",
    "              'Last score: {:5.2f} ({:5.2f} / {:5.2f})\\t' \\\n",
    "              'Moving average: {:5.3f}'\n",
    "              .format(i_episode,\n",
    "                      scores[-1], scores_episode[0], scores_episode[1],\n",
    "                      moving_avg[-1]),\n",
    "              end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        ## Check if the environment has been solved\n",
    "        if moving_avg[-1].mean() >= config.target_score \\\n",
    "                and i_episode >= config.target_episodes:\n",
    "            print('\\n\\nEnvironment solved in {:d} episodes!\\t' \\\n",
    "                  'Moving Average Score: {:.3f}'\n",
    "                  .format(i_episode-config.target_episodes, moving_avg[-1]))\n",
    "            break\n",
    "\n",
    "    print('\\n')\n",
    "    return scores, agents_scores, moving_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training phase using these settings:\n",
      "{'device': 'cpu'\n",
      " 'seed': 997\n",
      " 'target_score': 0.5\n",
      " 'target_episodes': 100\n",
      " 'max_episodes': 2000\n",
      " 'state_size': 24\n",
      " 'action_size': 2\n",
      " 'num_agents': 2\n",
      " 'actor_layers': [64, 64]\n",
      " 'critic_layers': [64, 64]\n",
      " 'actor_lr': 0.003\n",
      " 'critic_lr': 0.0004\n",
      " 'lr_sched_step': 1\n",
      " 'lr_sched_gamma': 0.2\n",
      " 'batch_normalization': True\n",
      " 'buffer_size': 1000000\n",
      " 'batch_size': 64\n",
      " 'gamma': 0.99\n",
      " 'tau': 0.008\n",
      " 'noise': True\n",
      " 'noise_theta': 0.9\n",
      " 'noise_sigma': 0.01} \n",
      "\n",
      "Episode    4\tLast score:  0.00 ( 0.00 / -0.01)\tMoving average: 0.000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16178/3764202112.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647327489/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions[i] = torch.as_tensor(e.actions)\n",
      "E1117 06:11:37.305590930   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.333197429   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.352488807   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.370980215   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.391092950   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.411586243   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.431767834   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100\tLast score:  0.00 (-0.01 /  0.00)\tMoving average: 0.007\n",
      "Episode  200\tLast score:  0.00 ( 0.00 / -0.01)\tMoving average: 0.006\n",
      "Episode  300\tLast score:  0.10 ( 0.10 / -0.01)\tMoving average: 0.030\n",
      "Episode  400\tLast score:  0.00 (-0.01 /  0.00)\tMoving average: 0.060\n",
      "Episode  500\tLast score:  0.00 (-0.01 /  0.00)\tMoving average: 0.063\n",
      "Episode  600\tLast score:  0.20 ( 0.09 /  0.20)\tMoving average: 0.092\n",
      "Episode  700\tLast score:  0.00 ( 0.00 / -0.01)\tMoving average: 0.077\n",
      "Episode  800\tLast score:  0.10 ( 0.10 / -0.01)\tMoving average: 0.074\n",
      "Episode  900\tLast score:  0.10 ( 0.10 / -0.01)\tMoving average: 0.091\n",
      "Episode  935\tLast score:  0.10 ( 0.09 /  0.10)\tMoving average: 0.098\n",
      "Decreasing learning rate ...\n",
      "Episode 1000\tLast score:  0.20 ( 0.19 /  0.20)\tMoving average: 0.123\n",
      "Episode 1100\tLast score:  0.09 ( 0.09 /  0.00)\tMoving average: 0.140\n",
      "Episode 1200\tLast score:  0.10 ( 0.10 /  0.09)\tMoving average: 0.124\n",
      "Episode 1300\tLast score:  0.10 ( 0.10 /  0.09)\tMoving average: 0.116\n",
      "Episode 1400\tLast score:  0.10 ( 0.09 /  0.10)\tMoving average: 0.113\n",
      "Episode 1487\tLast score:  0.19 ( 0.10 /  0.19)\tMoving average: 0.114"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the agent\n",
    "print('Starting training phase using these settings:')\n",
    "print(\"\\n '\".join(str(config.__dict__).split(\", '\")), '\\n')\n",
    "\n",
    "scores, agents_scores, moving_avg = train(env, maddpg, config.max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## Plot graphic of rewards\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Trace a line indicating the target value\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m target \u001b[38;5;241m=\u001b[39m [config\u001b[38;5;241m.\u001b[39mtarget_score] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mscores\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Graphic with the total rewards\u001b[39;00m\n\u001b[1;32m     10\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "## Plot graphic of rewards\n",
    "\n",
    "# Trace a line indicating the target value\n",
    "target = [config.target_score] * len(scores)\n",
    "\n",
    "# Graphic with the total rewards\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Plot of the rewards', fontsize='xx-large')\n",
    "ax.plot(scores, label='Score', color='Blue')\n",
    "ax.plot(moving_avg, label='Moving Average',\n",
    "        color='DarkOrange', linewidth=3)\n",
    "ax.plot(target, linestyle='--', color='LightCoral', linewidth=1 )\n",
    "ax.text(0, config.target_score, 'Target', color='LightCoral', fontsize='large')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Episode #')\n",
    "ax.legend(fontsize='xx-large')\n",
    "plt.show()\n",
    "\n",
    "# Graphics for each one of the agents\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8), sharex=True, sharey=True)\n",
    "fig.suptitle('Rewards for each one of the agents', fontsize='xx-large')\n",
    "axs = axs.flatten()\n",
    "for idx, (ax, s) in enumerate(zip(axs, np.transpose(agents_scores))):\n",
    "    ax.plot(s, label='Agent Score', color='DodgerBlue', zorder=1)\n",
    "    ax.plot(moving_avg, label='Moving Avg (Total)',\n",
    "            color='DarkOrange', linewidth=3, alpha=0.655, zorder=2)\n",
    "    ax.plot(target, linestyle='--', color='LightCoral', linewidth=1, zorder=0)\n",
    "    ax.text(0, config.target_score, 'Target',\n",
    "            color='LightCoral', fontsize='large')\n",
    "    ax.set_title('Agent #%d' % (idx+1))\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Episode #')\n",
    "    ax.label_outer()\n",
    "    ax.legend(fontsize='medium')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'maddpg_state_dict': maddpg.state_dict(),\n",
    "    'scores': scores,\n",
    "    'agents_scores': agents_scores,\n",
    "    'moving_avg': moving_avg}\n",
    "torch.save(checkpoint, 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the trained model\n",
    "def test(env, maddpg, max_episodes=3):\n",
    "    \"\"\"Test a Multi Agent Deep Deterministic Policy Gradients (MADDPG).\n",
    "    Params\n",
    "    ======\n",
    "        env (UnityEnvironment): environment for the agents\n",
    "        maddpg (MADDPG): the Multi Agent DDPG\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    ## Perform n_episodes of training\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        scores = np.zeros(config.num_agents)\n",
    "        while True:\n",
    "            states = env_info.vector_observations\n",
    "            actions = maddpg.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            scores += env_info.rewards\n",
    "            states = env_info.vector_observations\n",
    "            if any(env_info.local_done):\n",
    "                break\n",
    "        print('\\rEpisode {:4d}\\tScore: {:5.2f} ({:5.2f} / {:5.2f})\\t'\n",
    "              .format(i_episode, scores.max(), scores[0], scores[1]))\n",
    "\n",
    "checkpoint = torch.load('checkpoint.pt')\n",
    "config.noise = False\n",
    "maddpg = MultiAgentDDPG()\n",
    "maddpg.load_state_dict(checkpoint['maddpg_state_dict'])\n",
    "test(env, maddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (udacity_rl_new)",
   "language": "python",
   "name": "udacity_rl_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
