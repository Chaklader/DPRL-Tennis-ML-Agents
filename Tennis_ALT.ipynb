{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/DPRL-Tennis-ML-Agents/python')\n",
    "# sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "np.float_ = np.float64\n",
    "np.int_ = np.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/ubuntu/DPRL-Tennis-ML-Agents/Tennis_Linux/Tennis.x86_64\n",
      "Mono path[0] = '/home/ubuntu/DPRL-Tennis-ML-Agents/Tennis_Linux/Tennis_Data/Managed'\n",
      "Mono config path = '/home/ubuntu/DPRL-Tennis-ML-Agents/Tennis_Linux/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "\tScreenSelector.so\n",
      "Logging to /home/ubuntu/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\", no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 6):                                      # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     while True:\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#         env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Management\n",
    "\n",
    "This cell defines the configuration management system for the MADDPG tennis environment using a singleton pattern.\n",
    "\n",
    "## Purpose\n",
    "The `Config` class serves as a central configuration manager, ensuring consistent settings across all components of the MADDPG system.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Environment Settings\n",
    "- `device`: Computing device ('cpu' or 'cuda')\n",
    "- `seed`: Random seed for reproducibility\n",
    "- `target_score`: Score threshold for solving environment\n",
    "- `target_episodes`: Episodes to average score over\n",
    "- `max_episodes`: Maximum training episodes\n",
    "\n",
    "### State/Action Space\n",
    "- `state_size`: Dimension of state space\n",
    "- `action_size`: Dimension of action space\n",
    "- `num_agents`: Number of agents in environment\n",
    "\n",
    "### Network Architecture\n",
    "- `actor_layers`: Hidden layer sizes for actor network\n",
    "- `critic_layers`: Hidden layer sizes for critic network\n",
    "- `actor_lr`: Learning rate for actor network\n",
    "- `critic_lr`: Learning rate for critic network\n",
    "- `lr_sched_step`: Steps between learning rate updates\n",
    "- `lr_sched_gamma`: Learning rate decay factor\n",
    "- `batch_normalization`: Whether to use batch normalization\n",
    "\n",
    "### Experience Replay\n",
    "- `buffer_size`: Size of replay buffer\n",
    "- `batch_size`: Size of training batches\n",
    "- `gamma`: Discount factor\n",
    "- `tau`: Soft update interpolation factor\n",
    "\n",
    "### Exploration Settings\n",
    "- `noise`: Whether to add exploration noise\n",
    "- `noise_theta`: Ornstein-Uhlenbeck noise parameter\n",
    "- `noise_sigma`: Ornstein-Uhlenbeck noise parameter\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "config = Config()\n",
    "config.device = 'cuda'  # Set parameters individually\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __new__(self):\n",
    "        \"\"\"Define this class as a singleton\"\"\"\n",
    "        if not hasattr(self, 'instance'):\n",
    "            self.instance = super().__new__(self)\n",
    "\n",
    "            self.instance.device = None\n",
    "            self.instance.seed = None\n",
    "            self.instance.target_score = None\n",
    "            self.instance.target_episodes = None\n",
    "            self.instance.max_episodes = None\n",
    "\n",
    "            self.instance.state_size = None\n",
    "            self.instance.action_size = None\n",
    "            self.instance.num_agents = None\n",
    "\n",
    "            self.instance.actor_layers = None\n",
    "            self.instance.critic_layers = None\n",
    "            self.instance.actor_lr = None\n",
    "            self.instance.critic_lr = None\n",
    "            self.instance.lr_sched_step = None\n",
    "            self.instance.lr_sched_gamma = None\n",
    "\n",
    "            self.instance.batch_normalization = None\n",
    "\n",
    "            self.instance.buffer_size = None\n",
    "            self.instance.batch_size = None\n",
    "            self.instance.gamma = None\n",
    "            self.instance.tau = None\n",
    "\n",
    "            self.instance.noise = None\n",
    "            self.instance.noise_theta = None\n",
    "            self.instance.noise_sigma = None\n",
    "\n",
    "        return self.instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Neural Network Implementation\n",
    "\n",
    "This cell defines the base neural network class used as a foundation for both Actor and Critic networks in the MADDPG system.\n",
    "\n",
    "## Purpose\n",
    "The `BaseNN` class provides common functionality for neural network architectures, handling device configuration, initialization, and layer operations.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Core Functionality\n",
    "- Device configuration and random seed initialization\n",
    "- Dynamic layer management using ModuleList\n",
    "- Standardized parameter initialization\n",
    "\n",
    "### Methods\n",
    "- `create_fc_layer`: Creates and initializes fully connected layers\n",
    "- `reset_parameters`: Initializes weights using uniform distribution [-3e-3, 3e-3]\n",
    "\n",
    "### Attributes\n",
    "- `config`: Configuration singleton instance\n",
    "- `module_list`: Dynamic list storing network layers\n",
    "\n",
    "## Implementation Details\n",
    "- Inherits from `torch.nn.Module`\n",
    "- Uses DDPG-specific weight initialization range\n",
    "- Supports configurable device placement (CPU/CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BaseNN(nn.Module):\n",
    "    \"\"\"Base neural network class for Actor and Critic networks.\n",
    "    \n",
    "    This abstract base class provides common functionality for neural network \n",
    "    architectures used in the DDPG algorithm. It handles device configuration,\n",
    "    random seed initialization, and common layer operations.\n",
    "    \n",
    "    Attributes:\n",
    "        config (Config): Singleton configuration instance\n",
    "        module_list (nn.ModuleList): Dynamic list of network layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the base neural network.\n",
    "        \n",
    "        Sets up the network on the specified device, initializes random seed,\n",
    "        and creates an empty module list for storing layers.\n",
    "        \"\"\"\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.config = Config()\n",
    "        self.to(self.config.device)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        self.module_list = nn.ModuleList()\n",
    "\n",
    "    def create_fc_layer(self, nodes_in: int, nodes_out: int) -> None:\n",
    "        \"\"\"Create and initialize a fully connected layer.\n",
    "        \n",
    "        Args:\n",
    "            nodes_in: Number of input features\n",
    "            nodes_out: Number of output features\n",
    "        \"\"\"\n",
    "        layer = nn.Linear(nodes_in, nodes_out)\n",
    "        self.reset_parameters(layer)\n",
    "        self.module_list.append(layer)\n",
    "\n",
    "    def reset_parameters(self, layer: nn.Linear) -> None:\n",
    "        \"\"\"Initialize layer weights using uniform distribution.\n",
    "        \n",
    "        Args:\n",
    "            layer: Linear layer to initialize\n",
    "            \n",
    "        Note:\n",
    "            Uses uniform distribution in range [-3e-3, 3e-3] as per DDPG paper.\n",
    "        \"\"\"\n",
    "        layer.weight.data.uniform_(-3e-3, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network Implementation\n",
    "\n",
    "This cell defines the Actor network component of the DDPG algorithm, which learns the optimal policy for action selection.\n",
    "\n",
    "## Purpose\n",
    "The Actor network implements a deterministic policy function that maps states to continuous actions. It serves as the policy network in the DDPG algorithm, learning to select optimal actions directly from state observations.\n",
    "\n",
    "## Architecture\n",
    "- **Input Layer**: Accepts state observations (`state_size` dimensions)\n",
    "- **Hidden Layers**: Configurable via `actor_layers` parameter\n",
    "- **Output Layer**: Produces action values (`action_size` dimensions)\n",
    "- **Activations**: \n",
    "  - ReLU for hidden layers\n",
    "  - Tanh for output layer (bounds actions to [-1, 1])\n",
    "\n",
    "## Key Methods\n",
    "- `layers_nodes`: Generates layer size pairs for network construction\n",
    "- `forward`: Implements forward pass through the network\n",
    "\n",
    "## Implementation Details\n",
    "- Inherits from `BaseNN` for common neural network functionality\n",
    "- Uses dynamic layer creation based on configuration\n",
    "- Ensures bounded action outputs through tanh activation\n",
    "- Supports batch processing for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseNN):\n",
    "    \"\"\"Actor (policy) network that maps states to actions in DDPG.\n",
    "    \n",
    "    This network implements the policy function Œº(s|Œ∏_Œº) which maps states to \n",
    "    deterministic actions. It uses ReLU activation for hidden layers and tanh \n",
    "    for the output layer to bound actions to [-1, 1].\n",
    "    \n",
    "    Inherits from:\n",
    "        BaseNN: Base neural network class providing common functionality\n",
    "    \n",
    "    Architecture:\n",
    "        - Input layer: state_size neurons\n",
    "        - Hidden layers: Configured via actor_layers in Config\n",
    "        - Output layer: action_size neurons with tanh activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the actor network with configured architecture.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        for nodes_in, nodes_out in self.layers_nodes():\n",
    "            self.create_fc_layer(nodes_in, nodes_out)\n",
    "\n",
    "    def layers_nodes(self) -> zip:\n",
    "        \"\"\"Generate pairs of input/output sizes for network layers.\n",
    "        \n",
    "        Returns:\n",
    "            zip: Iterator of (input_size, output_size) tuples for each layer\n",
    "        \n",
    "        Note:\n",
    "            Layer sizes are determined by:\n",
    "            - Input: state_size\n",
    "            - Hidden: actor_layers from config\n",
    "            - Output: action_size\n",
    "        \"\"\"\n",
    "        nodes = []\n",
    "        nodes.append(self.config.state_size)\n",
    "        nodes.extend(self.config.actor_layers)\n",
    "        nodes.append(self.config.action_size)\n",
    "        nodes_in = nodes[:-1]\n",
    "        nodes_out = nodes[1:]\n",
    "        return zip(nodes_in, nodes_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor representing the state\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the action, bounded to [-1, 1]\n",
    "            \n",
    "        Note:\n",
    "            Uses ReLU activation for hidden layers and tanh for output layer\n",
    "            to bound actions to [-1, 1] range.\n",
    "        \"\"\"\n",
    "        for layer in self.module_list[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.module_list[-1](x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network Implementation\n",
    "\n",
    "This cell defines the Critic network component of the DDPG algorithm, which estimates Q-values for state-action pairs.\n",
    "\n",
    "## Purpose\n",
    "The Critic network implements the Q-function that estimates expected cumulative rewards. It evaluates the quality of actions in given states, providing value estimates that guide the Actor's policy updates.\n",
    "\n",
    "## Architecture\n",
    "- **Input Processing**:\n",
    "  - State input: `state_size * num_agents` dimensions\n",
    "  - Action input: `action_size * num_agents` dimensions\n",
    "  - Actions concatenated after first hidden layer\n",
    "- **Hidden Layers**: Configurable via `critic_layers` parameter\n",
    "- **Output**: Single Q-value neuron with sigmoid activation\n",
    "- **Optional**: Batch normalization after action concatenation\n",
    "\n",
    "## Key Methods\n",
    "- `layers_nodes`: Configures network layer dimensions\n",
    "- `forward`: Processes state-action pairs to produce Q-values\n",
    "\n",
    "## Implementation Details\n",
    "- Inherits from `BaseNN` for common functionality\n",
    "- Supports multi-agent state/action processing\n",
    "- Uses ReLU activations for hidden layers\n",
    "- Bounds Q-values to [0, 1] through sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(BaseNN):\n",
    "    \"\"\"Critic (value) network that maps state-action pairs to Q-values in DDPG.\n",
    "    \n",
    "    This network implements the Q-function Q(s,a|Œ∏_Q) which estimates the expected\n",
    "    cumulative reward for taking an action in a given state. It processes both \n",
    "    state and action inputs, with optional batch normalization.\n",
    "    \n",
    "    Inherits from:\n",
    "        BaseNN: Base neural network class providing common functionality\n",
    "    \n",
    "    Architecture:\n",
    "        - Input layer 1: state_size * num_agents neurons (state processing)\n",
    "        - Input layer 2: action_size * num_agents neurons (action processing)\n",
    "        - Hidden layers: Configured via critic_layers in Config\n",
    "        - Output layer: 1 neuron (Q-value) with sigmoid activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the critic network with configured architecture.\n",
    "        \n",
    "        If batch normalization is enabled in config, initializes a BatchNorm1d\n",
    "        layer after the action input concatenation.\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        for nodes_in, nodes_out in self.layers_nodes():\n",
    "            self.create_fc_layer(nodes_in, nodes_out)\n",
    "        if self.config.batch_normalization:\n",
    "            self.bn = nn.BatchNorm1d(self.module_list[1].in_features)\n",
    "\n",
    "    def layers_nodes(self) -> zip:\n",
    "        \"\"\"Generate pairs of input/output sizes for network layers.\n",
    "        \n",
    "        Returns:\n",
    "            zip: Iterator of (input_size, output_size) tuples for each layer\n",
    "            \n",
    "        Note:\n",
    "            Layer sizes account for:\n",
    "            - Combined state inputs from all agents\n",
    "            - Action inputs concatenated after first hidden layer\n",
    "            - Single Q-value output\n",
    "        \"\"\"\n",
    "        nodes = []\n",
    "        nodes.append(self.config.state_size * self.config.num_agents)\n",
    "        nodes.extend(self.config.critic_layers)\n",
    "        nodes.append(1)\n",
    "        nodes_in = nodes[:-1]\n",
    "        # Add action size to second layer's input\n",
    "        nodes_in[1] += self.config.num_agents * self.config.action_size\n",
    "        nodes_out = nodes[1:]\n",
    "        return zip(nodes_in, nodes_out)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            state: Input tensor representing the combined states of all agents\n",
    "            action: Input tensor representing the combined actions of all agents\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the Q-value estimate\n",
    "            \n",
    "        Note:\n",
    "            - First processes state through initial layer\n",
    "            - Concatenates action input\n",
    "            - Applies optional batch normalization\n",
    "            - Processes through remaining layers with ReLU activation\n",
    "            - Final sigmoid activation bounds output to [0, 1]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.module_list[0](state))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        if self.config.batch_normalization:\n",
    "            x = self.bn(x)\n",
    "        for layer in self.module_list[1:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.module_list[-1](x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "## Overview\n",
    "DDPG is an off-policy actor-critic algorithm designed for continuous action spaces, combining insights from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network).\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "### 1. Actor Network (Policy)\n",
    "The actor Œº(s|Œ∏Œº) implements a deterministic policy:\n",
    "```\n",
    "a = Œº(s|Œ∏Œº)\n",
    "```\n",
    "where:\n",
    "- s is the state\n",
    "- Œ∏Œº are the actor network parameters\n",
    "- a is the deterministic action\n",
    "\n",
    "### 2. Critic Network (Q-Value)\n",
    "The critic Q(s,a|Œ∏Q) estimates the action-value function:\n",
    "```\n",
    "Q(s,a|Œ∏Q) ‚âà ùîº[Rt|st=s, at=a]\n",
    "```\n",
    "where:\n",
    "- Rt = Œ£ Œ≥krt+k is the discounted future reward\n",
    "- Œ∏Q are the critic network parameters\n",
    "\n",
    "### 3. Learning Updates\n",
    "\n",
    "#### Critic Update\n",
    "The critic is updated to minimize the mean squared Bellman error (MSBE):\n",
    "```\n",
    "L(Œ∏Q) = ùîº[(Q(s,a|Œ∏Q) - y)¬≤]\n",
    "y = r + Œ≥Q(s',Œº(s'|Œ∏Œº')|Œ∏Q')\n",
    "```\n",
    "where:\n",
    "- Œ∏Q' and Œ∏Œº' are target network parameters\n",
    "- Œ≥ is the discount factor\n",
    "- r is the immediate reward\n",
    "\n",
    "#### Actor Update\n",
    "The actor is updated using the deterministic policy gradient theorem:\n",
    "```\n",
    "‚àáŒ∏ŒºJ ‚âà ùîº[‚àáaQ(s,a|Œ∏Q)|a=Œº(s) ‚àáŒ∏ŒºŒº(s|Œ∏Œº)]\n",
    "```\n",
    "\n",
    "### 4. Target Networks\n",
    "Target networks are updated using soft updates:\n",
    "```\n",
    "Œ∏' ‚Üê œÑŒ∏ + (1-œÑ)Œ∏'\n",
    "```\n",
    "where:\n",
    "- œÑ ‚â™ 1 is the soft update parameter\n",
    "- Œ∏' represents target network parameters\n",
    "- Œ∏ represents main network parameters\n",
    "\n",
    "### 5. Exploration\n",
    "Exploration is achieved using the Ornstein-Uhlenbeck process:\n",
    "```\n",
    "dx = Œ∏(Œº-x)dt + œÉdW\n",
    "```\n",
    "where:\n",
    "- Œ∏ controls mean reversion strength\n",
    "- Œº is the mean value\n",
    "- œÉ scales the noise\n",
    "- dW is a Wiener process\n",
    "\n",
    "## Key Features\n",
    "- Off-policy learning\n",
    "- Continuous action spaces\n",
    "- Experience replay for sample efficiency\n",
    "- Target networks for stability\n",
    "- Temporally correlated exploration noise\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DDPG Agent Implementation\n",
    "\n",
    "This cell defines the core DDPG agent class that interacts with the environment and manages the learning process.\n",
    "\n",
    "## Purpose\n",
    "The `Agent` class implements a Deep Deterministic Policy Gradients (DDPG) agent, managing both actor and critic networks, handling action selection, and coordinating the learning process.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Networks\n",
    "- **Actor Networks**:\n",
    "  - Local network for current policy\n",
    "  - Target network for stable learning\n",
    "  - Adam optimizer with learning rate scheduling\n",
    "  \n",
    "- **Critic Networks**:\n",
    "  - Local network for current Q-value estimation\n",
    "  - Target network for stable learning\n",
    "  - Adam optimizer with learning rate scheduling\n",
    "\n",
    "### Methods\n",
    "- `soft_update`: Gradually updates target networks using œÑ parameter\n",
    "- `act`: Selects actions using current policy and optional exploration\n",
    "- `lr_step`: Updates learning rates using schedulers\n",
    "- `reset_noise`: Resets exploration noise process\n",
    "\n",
    "## Implementation Details\n",
    "- Uses Ornstein-Uhlenbeck noise for exploration\n",
    "- Implements target networks with soft updates\n",
    "- Supports learning rate decay through schedulers\n",
    "- Clips actions to [-1, 1] range\n",
    "- Handles device placement (CPU/CUDA)\n",
    "\n",
    "## Note\n",
    "The agent implements key DDPG features including:\n",
    "- Experience replay (through buffer)\n",
    "- Target networks for stability\n",
    "- Continuous action space handling\n",
    "- Exploration vs exploitation balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"DDPG agent that interacts with and learns from the environment.\n",
    "    \n",
    "    This class implements a Deep Deterministic Policy Gradients (DDPG) agent\n",
    "    with both actor and critic networks, including their local and target versions.\n",
    "    It handles action selection, network updates, and exploration noise.\n",
    "    \n",
    "    Attributes:\n",
    "        actor_local (Actor): Local actor network for policy\n",
    "        actor_target (Actor): Target actor network for stable learning\n",
    "        actor_optimizer (Adam): Optimizer for actor network\n",
    "        actor_lr_scheduler (StepLR): Learning rate scheduler for actor\n",
    "        critic_local (Critic): Local critic network for Q-value estimation\n",
    "        critic_target (Critic): Target critic network for stable learning\n",
    "        critic_optimizer (Adam): Optimizer for critic network\n",
    "        critic_lr_scheduler (StepLR): Learning rate scheduler for critic\n",
    "        noise (OUNoise): Ornstein-Uhlenbeck noise process for exploration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize agent with networks, optimizers, and noise process.\"\"\"\n",
    "        self.config = Config()\n",
    "        random.seed(self.config.seed)\n",
    "\n",
    "        # Initialize and synchronize actor networks\n",
    "        self.actor_local = Actor()\n",
    "        self.actor_target = Actor()\n",
    "        self.actor_target.load_state_dict(self.actor_local.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(\n",
    "            self.actor_local.parameters(),\n",
    "            lr=self.config.actor_lr)\n",
    "        self.actor_lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.actor_optimizer,\n",
    "            step_size=self.config.lr_sched_step,\n",
    "            gamma=self.config.lr_sched_gamma)\n",
    "\n",
    "        # Initialize and synchronize critic networks\n",
    "        self.critic_local = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_target.load_state_dict(self.critic_local.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            self.critic_local.parameters(),\n",
    "            lr=self.config.critic_lr)\n",
    "        self.critic_lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.critic_optimizer,\n",
    "            step_size=self.config.lr_sched_step,\n",
    "            gamma=self.config.lr_sched_gamma)\n",
    "\n",
    "        self.noise = OUNoise()\n",
    "\n",
    "    def soft_update(self) -> None:\n",
    "        \"\"\"Soft update of target network parameters.\n",
    "        \n",
    "        Œ∏_target = œÑ*Œ∏_local + (1 - œÑ)*Œ∏_target\n",
    "        \n",
    "        Uses the config.tau parameter to interpolate between local and target\n",
    "        network parameters, providing stable learning.\n",
    "        \"\"\"\n",
    "        tau = self.config.tau\n",
    "        for target_param, local_param in zip(self.actor_target.parameters(),\n",
    "                                           self.actor_local.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, local_param in zip(self.critic_target.parameters(),\n",
    "                                           self.critic_local.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action for the given state using current policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Selected action, with optional exploration noise\n",
    "            \n",
    "        Note:\n",
    "            Actions are clipped to [-1, 1] range after adding exploration noise.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.actor_local.eval()\n",
    "            state = torch.from_numpy(state).float()\n",
    "            state.to(self.config.device)\n",
    "            action = self.actor_local(state).data.cpu().numpy()\n",
    "            self.actor_local.train()\n",
    "\n",
    "        if self.config.noise:\n",
    "            action += self.noise.sample()\n",
    "            np.clip(action, a_min=-1, a_max=1, out=action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def lr_step(self) -> None:\n",
    "        \"\"\"Update learning rates according to schedulers.\"\"\"\n",
    "        self.actor_lr_scheduler.step()\n",
    "        self.critic_lr_scheduler.step()\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        \"\"\"Reset the exploration noise process.\"\"\"\n",
    "        self.noise.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ornstein-Uhlenbeck Noise Implementation\n",
    "\n",
    "This cell implements the Ornstein-Uhlenbeck noise process used for exploration in continuous action spaces.\n",
    "\n",
    "## Purpose\n",
    "The `OUNoise` class generates temporally correlated noise samples that help agents explore continuous action spaces effectively, particularly in physical control tasks where actions should change smoothly over time.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Parameters\n",
    "- `mu`: Mean value of the noise process\n",
    "- `theta`: Mean reversion strength\n",
    "- `sigma`: Scale of the random noise\n",
    "\n",
    "### Methods\n",
    "- `reset`: Resets noise state to initial mean value\n",
    "- `sample`: Generates next noise sample using OU process\n",
    "\n",
    "## Mathematical Model\n",
    "The Ornstein-Uhlenbeck process follows the equation:\n",
    "```\n",
    "dx = Œ∏(Œº - x)dt + œÉdW\n",
    "```\n",
    "where:\n",
    "- `Œ∏` controls the strength of mean reversion\n",
    "- `Œº` is the mean value\n",
    "- `œÉ` scales the random process\n",
    "- `dW` is a Wiener process\n",
    "\n",
    "## Implementation Details\n",
    "- Configurable through Config singleton\n",
    "- Maintains state between samples for temporal correlation\n",
    "- Supports multi-dimensional action spaces\n",
    "- Uses uniform random sampling for noise generation\n",
    "\n",
    "## Note\n",
    "The OU process is particularly suitable for physical control problems as it generates smooth noise that respects the system's inertia.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck noise process for exploration in continuous action spaces.\n",
    "    \n",
    "    Implements temporally correlated noise using the Ornstein-Uhlenbeck process,\n",
    "    which is particularly suitable for physical control problems with inertia.\n",
    "    \n",
    "    Attributes:\n",
    "        mu (np.ndarray): Mean value of the process\n",
    "        state (np.ndarray): Current state of the noise process\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mu: float = 0.):\n",
    "        \"\"\"Initialize the noise process.\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean value of the noise process (default: 0.0)\n",
    "        \"\"\"\n",
    "        self.config = Config()\n",
    "        random.seed(self.config.seed)\n",
    "        self.mu = mu * np.ones(self.config.action_size)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset noise state to initial mean value.\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self) -> np.ndarray:\n",
    "        \"\"\"Generate next noise sample using the OU process.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Noise sample to add to the action\n",
    "            \n",
    "        Note:\n",
    "            Uses config.noise_theta for mean reversion strength and\n",
    "            config.noise_sigma for noise scale.\n",
    "        \"\"\"\n",
    "        x = self.state\n",
    "        random_array = [random.random() for i in range(len(x))]\n",
    "        dx = self.config.noise_theta * (self.mu - x) \\\n",
    "             + self.config.noise_sigma * np.array(random_array)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay Buffer Implementation\n",
    "\n",
    "This cell implements the replay buffer for storing and sampling experiences during MADDPG training.\n",
    "\n",
    "## Purpose\n",
    "The `ReplayBuffer` class manages experience storage and retrieval for training, implementing experience replay to improve learning stability and sample efficiency in multi-agent environments.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Data Structures\n",
    "- `deque`: Fixed-size double-ended queue for efficient experience storage\n",
    "- `namedtuple`: Structured storage for individual transitions\n",
    "- `torch.Tensor`: Batched experience tensors for training\n",
    "\n",
    "### Methods\n",
    "- `store`: Adds new experiences to memory\n",
    "- `sample`: Randomly samples batches of experiences\n",
    "- `create_tensor`: Creates appropriately shaped tensors for batched data\n",
    "\n",
    "## Implementation Details\n",
    "- Supports multi-agent experiences\n",
    "- Handles batched data with configurable dimensions\n",
    "- Manages tensor creation with proper shapes:\n",
    "  - States: `(batch_size, num_agents, state_size)`\n",
    "  - Actions: `(batch_size, num_agents, action_size)`\n",
    "  - Rewards: `(batch_size, num_agents)`\n",
    "- Ensures consistent device placement and data types\n",
    "\n",
    "## Note\n",
    "The buffer is essential for:\n",
    "- Breaking temporal correlations in training data\n",
    "- Enabling efficient batch processing\n",
    "- Supporting multi-agent experience sharing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples for DDPG training.\n",
    "    \n",
    "    This class implements experience replay, storing transitions (state, action, \n",
    "    reward, next_state) and enabling random sampling for training. It supports\n",
    "    multi-agent environments by handling batched experiences.\n",
    "    \n",
    "    Attributes:\n",
    "        memory (deque): Double-ended queue storing experience tuples\n",
    "        experience (namedtuple): Template for storing single transitions\n",
    "        config (Config): Configuration singleton instance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize replay buffer with configured size and random seed.\"\"\"\n",
    "        self.config = Config()\n",
    "        random.seed(self.config.seed)\n",
    "        self.memory = deque(maxlen=self.config.buffer_size)\n",
    "\n",
    "        self.experience = namedtuple(\n",
    "            'Experience',\n",
    "            field_names=['state', 'actions', 'rewards', 'next_state'])\n",
    "\n",
    "    def store(self, state: list, actions: list, reward: list, \n",
    "              next_state: list) -> None:\n",
    "        \"\"\"Add a new experience to memory.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observation\n",
    "            actions: Actions taken by agents\n",
    "            reward: Rewards received\n",
    "            next_state: Next state observation\n",
    "        \"\"\"\n",
    "        e = self.experience(state, actions, reward, next_state)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self) -> tuple:\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Batch of (states, actions, rewards, next_states) as tensors\n",
    "            \n",
    "        Note:\n",
    "            Returns tensors of shape (batch_size, num_agents, feature_dim)\n",
    "            where feature_dim varies for different components:\n",
    "            - states/next_states: state_size\n",
    "            - actions: action_size\n",
    "            - rewards: 1\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, self.config.batch_size)\n",
    "\n",
    "        states = self.create_tensor(dim=self.config.state_size)\n",
    "        actions = self.create_tensor(dim=self.config.action_size)\n",
    "        rewards = self.create_tensor()\n",
    "        next_states = self.create_tensor(dim=self.config.state_size)\n",
    "        \n",
    "        for i, e in enumerate(experiences):\n",
    "            states[i] = torch.as_tensor(e.state)\n",
    "            actions[i] = torch.as_tensor(e.actions)\n",
    "            rewards[i] = torch.as_tensor(e.rewards)\n",
    "            next_states[i] = torch.as_tensor(e.next_state)\n",
    "            \n",
    "        return (states, actions, rewards, next_states)\n",
    "\n",
    "    def create_tensor(self, dim: int = 0) -> torch.Tensor:\n",
    "        \"\"\"Create an empty tensor for storing batched experiences.\n",
    "        \n",
    "        Args:\n",
    "            dim: Feature dimension for the tensor (default: 0 for rewards)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Empty tensor of appropriate shape\n",
    "            \n",
    "        Note:\n",
    "            Shape is (batch_size, num_agents, dim) for state/action tensors\n",
    "            or (batch_size, num_agents) for reward tensors\n",
    "        \"\"\"\n",
    "        batch_size = self.config.batch_size\n",
    "        num_agents = self.config.num_agents\n",
    "        \n",
    "        if dim > 0:\n",
    "            size = (batch_size, num_agents, dim)\n",
    "        else:\n",
    "            size = (batch_size, num_agents)\n",
    "\n",
    "        tensor = torch.empty(\n",
    "            size=size, \n",
    "            dtype=torch.float,\n",
    "            device=self.config.device,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "## Overview\n",
    "MADDPG extends DDPG to multi-agent environments using centralized training with decentralized execution. It addresses non-stationarity in multi-agent learning by considering all agents' actions during training.\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "### 1. Multi-Agent Setup\n",
    "For N agents with policies œÄ = {œÄ1, ..., œÄN}, states s = {s1, ..., sN}, and actions a = {a1, ..., aN}:\n",
    "\n",
    "### 2. Centralized Critic\n",
    "Each agent i has a critic that takes all agents' information:\n",
    "````\n",
    "Qi(s, a1, ..., aN|Œ∏Qi) ‚âà ùîº[Rit|s, a1, ..., aN]\n",
    "````\n",
    "\n",
    "where:\n",
    "- Rit is agent i's cumulative discounted reward\n",
    "- Œ∏Qi are critic parameters for agent i\n",
    "\n",
    "### 3. Decentralized Actor\n",
    "Each agent's actor only uses local observations:\n",
    "````\n",
    "Œºi(si|Œ∏Œºi) ‚Üí ai\n",
    "````\n",
    "\n",
    "where:\n",
    "- si is agent i's local observation\n",
    "- Œ∏Œºi are actor parameters for agent i\n",
    "\n",
    "### 4. Learning Updates\n",
    "\n",
    "#### Critic Update\n",
    "For each agent i:\n",
    "````\n",
    "L(Œ∏Qi) = ùîº[(Qi(s, a1, ..., aN) - yi)¬≤]\n",
    "yi = ri + Œ≥Qi'(s', Œº1'(s1'), ..., ŒºN'(sN'))\n",
    "````\n",
    "\n",
    "#### Actor Update\n",
    "Policy gradient for agent i:\n",
    "````\n",
    "‚àáŒ∏ŒºiJ ‚âà ùîº[‚àáaiQi(s, a1, ..., aN)|aj=Œºj(sj) ‚àáŒ∏ŒºiŒºi(si)]\n",
    "````\n",
    "\n",
    "## Key Distinctions from DDPG\n",
    "\n",
    "### 1. Critic Architecture\n",
    "- **DDPG**: Q(s, a) for single agent\n",
    "- **MADDPG**: Qi(s, a1, ..., aN) considers all agents' actions\n",
    "\n",
    "### 2. Training Process\n",
    "- **DDPG**: Single agent learning\n",
    "- **MADDPG**: Concurrent learning with shared information\n",
    "\n",
    "### 3. Non-stationarity\n",
    "- **DDPG**: Static environment\n",
    "- **MADDPG**: Handles changing policies of other agents\n",
    "\n",
    "### 4. Information Access\n",
    "- **DDPG**: Full observation during training and execution\n",
    "- **MADDPG**: \n",
    "  - Training: Full observation of all agents\n",
    "  - Execution: Only local observations\n",
    "\n",
    "### 5. Policy Updates\n",
    "- **DDPG**: Independent policy updates\n",
    "- **MADDPG**: Coordinated updates considering other agents\n",
    "\n",
    "## Advantages of MADDPG\n",
    "1. Better handling of non-stationary environments\n",
    "2. Improved coordination among agents\n",
    "3. More stable learning in competitive/cooperative scenarios\n",
    "4. Scalability to multiple agents\n",
    "\n",
    "## Challenges\n",
    "1. Increased computational complexity\n",
    "2. Higher memory requirements\n",
    "3. Need for careful hyperparameter tuning\n",
    "4. Potential communication overhead during training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-Agent DDPG Implementation\n",
    "\n",
    "This cell implements the core MADDPG (Multi-Agent Deep Deterministic Policy Gradient) system that coordinates multiple DDPG agents.\n",
    "\n",
    "## Purpose\n",
    "The `MultiAgentDDPG` class manages the training and coordination of multiple DDPG agents in a shared environment, implementing centralized training with decentralized execution.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Agent Management\n",
    "- Maintains list of DDPG agents\n",
    "- Coordinates shared experience buffer\n",
    "- Handles synchronized learning updates\n",
    "\n",
    "### Action Selection\n",
    "- `act`: Gets decentralized actions from each agent\n",
    "- `actions_target`: Computes target actions using target networks\n",
    "- `actions_local`: Manages action gradients for policy updates\n",
    "\n",
    "### Learning Process\n",
    "- **Experience Collection**:\n",
    "  - Stores transitions in shared buffer\n",
    "  - Triggers learning when enough samples available\n",
    "\n",
    "- **Update Procedure**:\n",
    "  1. Sample shared experiences\n",
    "  2. Update each agent's critic (Q-function)\n",
    "  3. Update each agent's actor (policy)\n",
    "  4. Soft update target networks\n",
    "\n",
    "### Utilities\n",
    "- State dictionary management for saving/loading\n",
    "- Learning rate scheduling\n",
    "- Noise process management\n",
    "\n",
    "## Implementation Details\n",
    "- Supports batch processing for efficient learning\n",
    "- Handles gradient flow control for actor updates\n",
    "- Implements centralized critic updates\n",
    "- Manages device placement for tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiAgentDDPG:\n",
    "    \"\"\"Multi-agent DDPG implementation for coordinated training of multiple agents.\n",
    "    \n",
    "    This class manages multiple DDPG agents in a shared environment, handling their\n",
    "    interactions, training, and synchronization. It implements experience replay\n",
    "    and coordinated learning updates for all agents.\n",
    "    \n",
    "    Attributes:\n",
    "        agents (list[Agent]): List of DDPG agents\n",
    "        buffer (ReplayBuffer): Shared experience replay buffer\n",
    "        config (Config): Configuration singleton instance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize multi-agent system with configured number of agents.\"\"\"\n",
    "        self.config = Config()\n",
    "        self.agents = [Agent() for _ in range(self.config.num_agents)]\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "    def act(self, state: list) -> list:\n",
    "        \"\"\"Get actions from all agents based on their current states.\n",
    "        \n",
    "        Args:\n",
    "            state: List of state observations for each agent\n",
    "            \n",
    "        Returns:\n",
    "            list: Actions chosen by each agent\n",
    "        \"\"\"\n",
    "        actions = [agent.act(obs) for agent, obs in zip(self.agents, state)]\n",
    "        return actions\n",
    "\n",
    "    def actions_target(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get target actions for all agents using their target networks.\n",
    "        \n",
    "        Args:\n",
    "            states: Batch of state observations (batch_size, num_agents, state_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Target actions (batch_size, num_agents, action_size)\n",
    "        \"\"\"\n",
    "        batch_size = self.config.batch_size\n",
    "        num_agents = self.config.num_agents\n",
    "        action_size = self.config.action_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions = torch.empty(\n",
    "                (batch_size, num_agents, action_size),\n",
    "                device=self.config.device)\n",
    "            for idx, agent in enumerate(self.agents):\n",
    "                actions[:,idx] = agent.actor_target(states[:,idx])\n",
    "        return actions\n",
    "\n",
    "    def actions_local(self, states: torch.Tensor, agent_id: int) -> torch.Tensor:\n",
    "        \"\"\"Get local actions for all agents, with gradients only for specified agent.\n",
    "        \n",
    "        Args:\n",
    "            states: Batch of state observations (batch_size, num_agents, state_size)\n",
    "            agent_id: ID of agent being updated\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Local actions (batch_size, num_agents, action_size)\n",
    "        \"\"\"\n",
    "        batch_size = self.config.batch_size\n",
    "        num_agents = self.config.num_agents\n",
    "        action_size = self.config.action_size\n",
    "\n",
    "        actions = torch.empty(\n",
    "            (batch_size, num_agents, action_size),\n",
    "            device=self.config.device)\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            action = agent.actor_local(states[:,idx])\n",
    "            if not idx == agent_id:\n",
    "                action.detach()\n",
    "            actions[:,idx] = action\n",
    "        return actions\n",
    "\n",
    "    def store(self, state: list, actions: list, rewards: list, \n",
    "              next_state: list) -> None:\n",
    "        \"\"\"Store experience in replay buffer and trigger learning if ready.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state observations\n",
    "            actions: Actions taken by agents\n",
    "            rewards: Rewards received\n",
    "            next_state: Next state observations\n",
    "        \"\"\"\n",
    "        self.buffer.store(state, actions, rewards, next_state)\n",
    "        if len(self.buffer) >= self.config.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self) -> None:\n",
    "        \"\"\"Update all agents using shared experiences.\n",
    "        \n",
    "        Implements the MADDPG learning algorithm:\n",
    "        1. Sample shared experiences\n",
    "        2. For each agent:\n",
    "            - Update critic using all agents' actions\n",
    "            - Update actor using policy gradient\n",
    "        3. Soft update target networks\n",
    "        \"\"\"\n",
    "        batch_size = self.config.batch_size\n",
    "        for agent_id, agent in enumerate(self.agents):\n",
    "            # Sample and prepare batch\n",
    "            states, actions, rewards, next_states = self.buffer.sample()\n",
    "            obs = states.view(batch_size, -1)\n",
    "            actions = actions.view(batch_size, -1)\n",
    "            next_obs = next_states.view(batch_size, -1)\n",
    "            r = rewards[:,agent_id].unsqueeze_(1)\n",
    "\n",
    "            # Update critic\n",
    "            with torch.no_grad():\n",
    "                next_actions = self.actions_target(next_states)\n",
    "                next_actions = next_actions.view(batch_size, -1)\n",
    "                next_q_val = agent.critic_target(next_obs, next_actions)\n",
    "                y = r + self.config.gamma * next_q_val\n",
    "            \n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            q_value_predicted = agent.critic_local(obs, actions)\n",
    "            critic_loss = F.mse_loss(q_value_predicted, y)\n",
    "            critic_loss.backward()\n",
    "            agent.critic_optimizer.step()\n",
    "\n",
    "            # Update actor\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            actions_local = self.actions_local(states, agent_id)\n",
    "            actions_local = actions_local.view(batch_size, -1)\n",
    "            actor_loss = -agent.critic_local(obs, actions_local).mean()\n",
    "            actor_loss.backward()\n",
    "            agent.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        for agent in self.agents:\n",
    "            agent.soft_update()\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        \"\"\"Reset exploration noise for all agents.\"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.reset_noise()\n",
    "\n",
    "    def state_dict(self) -> list:\n",
    "        \"\"\"Get state dictionaries of all agents' local actors.\n",
    "        \n",
    "        Returns:\n",
    "            list: State dictionaries for each agent's actor network\n",
    "        \"\"\"\n",
    "        return [agent.actor_local.state_dict() for agent in self.agents]\n",
    "\n",
    "    def load_state_dict(self, state_dicts: list) -> None:\n",
    "        \"\"\"Load saved state dictionaries into agents' local actors.\n",
    "        \n",
    "        Args:\n",
    "            state_dicts: List of state dictionaries for each agent\n",
    "        \"\"\"\n",
    "        for agent, state_dict in zip(self.agents, state_dicts):\n",
    "            agent.actor_local.load_state_dict(state_dict)\n",
    "\n",
    "    def lr_step(self) -> None:\n",
    "        \"\"\"Update learning rates for all agents.\"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.lr_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Initialization\n",
    "\n",
    "This cell sets up the configuration parameters for the MADDPG training system.\n",
    "\n",
    "## Environment Settings\n",
    "- Random seed: 997\n",
    "- Computing device: CPU\n",
    "- Target score: 0.5\n",
    "- Target episodes: 100\n",
    "- Maximum episodes: 2000\n",
    "\n",
    "## Network Architecture\n",
    "- Actor hidden layers: [64, 64]\n",
    "- Critic hidden layers: [64, 64]\n",
    "- Batch normalization: Enabled\n",
    "\n",
    "## Learning Parameters\n",
    "- Actor learning rate: 3e-3\n",
    "- Critic learning rate: 4e-4\n",
    "- Learning rate step size: 1\n",
    "- Learning rate decay: 0.2\n",
    "- Discount factor (gamma): 0.99\n",
    "- Soft update factor (tau): 8e-3\n",
    "\n",
    "## Experience Replay\n",
    "- Buffer size: 1,000,000\n",
    "- Batch size: 64\n",
    "\n",
    "## Exploration Settings\n",
    "- Noise enabled: True\n",
    "- OU noise theta: 0.9\n",
    "- OU noise sigma: 0.01\n",
    "\n",
    "## Environment Dimensions\n",
    "- Number of agents: (from environment)\n",
    "- Action size: (from environment)\n",
    "- State size: (from environment)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize values in Config\n",
    "config = Config()\n",
    "config.seed = 997\n",
    "config.device = 'cpu'\n",
    "config.target_score = 0.5\n",
    "config.target_episodes = 100\n",
    "\n",
    "config.num_agents = num_agents\n",
    "config.action_size = action_size\n",
    "config.state_size = state_size\n",
    "\n",
    "config.actor_layers = [64, 64]\n",
    "config.critic_layers = [64, 64]\n",
    "config.actor_lr = 3e-3\n",
    "config.critic_lr = 4e-4\n",
    "config.lr_sched_step = 1\n",
    "config.lr_sched_gamma = 0.2\n",
    "config.batch_normalization = True\n",
    "\n",
    "config.buffer_size = int(1e6)\n",
    "config.batch_size = 64\n",
    "config.gamma = 0.99\n",
    "config.tau = 8e-3\n",
    "config.noise = True\n",
    "config.noise_theta = 0.9\n",
    "config.noise_sigma = 0.01\n",
    "\n",
    "config.max_episodes = 2000\n",
    "\n",
    "# Instantiate a Multi Agent\n",
    "maddpg = MultiAgentDDPG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training Implementation\n",
    "\n",
    "This cell implements the main training loop for the MADDPG system.\n",
    "\n",
    "## Purpose\n",
    "The `train` function executes the core training process, managing episode execution, performance tracking, and learning rate adjustments.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Score Tracking\n",
    "- Maximum scores per episode\n",
    "- Individual agent scores\n",
    "- Moving average over target episodes\n",
    "\n",
    "### Learning Rate Management\n",
    "- Predefined checkpoints: [0.1, 0.2, 0.4, 0.6]\n",
    "- Automatic rate reduction at performance thresholds\n",
    "\n",
    "### Training Process\n",
    "1. **Episode Initialization**\n",
    "   - Reset environment and noise\n",
    "   - Initialize state tracking\n",
    "\n",
    "2. **Episode Execution**\n",
    "   - Action selection and execution\n",
    "   - Experience collection\n",
    "   - State and reward tracking\n",
    "\n",
    "3. **Performance Monitoring**\n",
    "   - Score tracking and averaging\n",
    "   - Learning rate adjustment\n",
    "   - Progress reporting\n",
    "\n",
    "4. **Termination Conditions**\n",
    "   - Maximum episodes reached\n",
    "   - Environment solved (target score achieved)\n",
    "\n",
    "## Implementation Details\n",
    "- Supports Unity ML-Agents environment\n",
    "- Handles multi-agent coordination\n",
    "- Implements adaptive learning rates\n",
    "- Provides detailed progress tracking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: UnityEnvironment, maddpg: MultiAgentDDPG, \n",
    "          max_episodes: int = 5000) -> tuple:\n",
    "    \"\"\"Train a Multi-Agent Deep Deterministic Policy Gradients (MADDPG) system.\n",
    "    \n",
    "    This function implements the main training loop for the MADDPG algorithm,\n",
    "    handling episode execution, score tracking, and learning rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        env: Unity environment for agent interaction\n",
    "        maddpg: Multi-agent DDPG system to be trained\n",
    "        max_episodes: Maximum number of training episodes (default: 5000)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Contains:\n",
    "            - list: Maximum scores per episode\n",
    "            - list: Individual agent scores per episode\n",
    "            - list: Moving average scores\n",
    "            \n",
    "    Note:\n",
    "        Training continues until either:\n",
    "        1. Maximum episodes reached\n",
    "        2. Environment solved (moving average >= target_score for target_episodes)\n",
    "        \n",
    "        Learning rate is decreased at predefined score checkpoints: [0.1, 0.2, 0.4, 0.6]\n",
    "    \"\"\"\n",
    "    # Initialize score tracking\n",
    "    scores = []          # Maximum scores per episode\n",
    "    agents_scores = []   # Individual agent scores\n",
    "    moving_avg = []      # Moving average of scores\n",
    "    \n",
    "    # Set up learning rate adjustment checkpoints\n",
    "    avg_checkpoints = iter([0.1, 0.2, 0.4, 0.6])\n",
    "    next_avg_checkpoint = next(avg_checkpoints)\n",
    "    \n",
    "    # Main training loop\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        # Reset environment and noise for new episode\n",
    "        maddpg.reset_noise()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        scores_episode = np.zeros(config.num_agents)\n",
    "        \n",
    "        # Episode execution loop\n",
    "        while True:\n",
    "            # Execute environment step\n",
    "            actions = maddpg.act(state)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            # Collect step information\n",
    "            rewards = env_info.rewards\n",
    "            next_state = env_info.vector_observations\n",
    "            \n",
    "            # Store experience and update state/scores\n",
    "            maddpg.store(state, actions, rewards, next_state)\n",
    "            state = next_state\n",
    "            scores_episode += rewards\n",
    "            \n",
    "            if any(env_info.local_done):\n",
    "                break\n",
    "        \n",
    "        # Update tracking metrics\n",
    "        agents_scores.append(scores_episode)\n",
    "        scores.append(scores_episode.max())\n",
    "        moving_avg.append(np.mean(scores[-config.target_episodes:]))\n",
    "        \n",
    "        # Check for learning rate adjustment\n",
    "        if moving_avg[-1] >= next_avg_checkpoint:\n",
    "            print('\\nDecreasing learning rate ...')\n",
    "            maddpg.lr_step()\n",
    "            try:\n",
    "                next_avg_checkpoint = next(avg_checkpoints)\n",
    "            except StopIteration:\n",
    "                pass\n",
    "        \n",
    "        # Print progress\n",
    "        print('\\rEpisode {:4d}\\t'\n",
    "              'Last score: {:5.2f} ({:5.2f} / {:5.2f})\\t'\n",
    "              'Moving average: {:5.3f}'\n",
    "              .format(i_episode,\n",
    "                     scores[-1], scores_episode[0], scores_episode[1],\n",
    "                     moving_avg[-1]),\n",
    "              end='')\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print()\n",
    "        \n",
    "        # Check if environment is solved\n",
    "        if (moving_avg[-1].mean() >= config.target_score and \n",
    "            i_episode >= config.target_episodes):\n",
    "            print('\\n\\nEnvironment solved in {:d} episodes!\\t'\n",
    "                  'Moving Average Score: {:.3f}'\n",
    "                  .format(i_episode - config.target_episodes, moving_avg[-1]))\n",
    "            break\n",
    "    \n",
    "    print('\\n')\n",
    "    return scores, agents_scores, moving_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Execution\n",
    "\n",
    "This cell initiates the MADDPG training process, configures matplotlib for inline plotting, and executes the main training loop with the specified configuration parameters. The training progress and final results are stored in variables tracking overall scores, individual agent performances, and moving averages.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training phase using these settings:\n",
      "{'device': 'cpu'\n",
      " 'seed': 997\n",
      " 'target_score': 0.5\n",
      " 'target_episodes': 100\n",
      " 'max_episodes': 2000\n",
      " 'state_size': 24\n",
      " 'action_size': 2\n",
      " 'num_agents': 2\n",
      " 'actor_layers': [64, 64]\n",
      " 'critic_layers': [64, 64]\n",
      " 'actor_lr': 0.003\n",
      " 'critic_lr': 0.0004\n",
      " 'lr_sched_step': 1\n",
      " 'lr_sched_gamma': 0.2\n",
      " 'batch_normalization': True\n",
      " 'buffer_size': 1000000\n",
      " 'batch_size': 64\n",
      " 'gamma': 0.99\n",
      " 'tau': 0.008\n",
      " 'noise': True\n",
      " 'noise_theta': 0.9\n",
      " 'noise_sigma': 0.01} \n",
      "\n",
      "Episode    4\tLast score:  0.00 ( 0.00 / -0.01)\tMoving average: 0.000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16178/3764202112.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647327489/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions[i] = torch.as_tensor(e.actions)\n",
      "E1117 06:11:37.305590930   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.333197429   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.352488807   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.370980215   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.391092950   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.411586243   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E1117 06:11:37.431767834   16178 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  100\tLast score:  0.00 (-0.01 /  0.00)\tMoving average: 0.007\n",
      "Episode  200\tLast score:  0.00 ( 0.00 / -0.01)\tMoving average: 0.006\n",
      "Episode  300\tLast score:  0.10 ( 0.10 / -0.01)\tMoving average: 0.030\n",
      "Episode  400\tLast score:  0.00 (-0.01 /  0.00)\tMoving average: 0.060\n",
      "Episode  500\tLast score:  0.00 (-0.01 /  0.00)\tMoving average: 0.063\n",
      "Episode  600\tLast score:  0.20 ( 0.09 /  0.20)\tMoving average: 0.092\n",
      "Episode  700\tLast score:  0.00 ( 0.00 / -0.01)\tMoving average: 0.077\n",
      "Episode  800\tLast score:  0.10 ( 0.10 / -0.01)\tMoving average: 0.074\n",
      "Episode  900\tLast score:  0.10 ( 0.10 / -0.01)\tMoving average: 0.091\n",
      "Episode  935\tLast score:  0.10 ( 0.09 /  0.10)\tMoving average: 0.098\n",
      "Decreasing learning rate ...\n",
      "Episode 1000\tLast score:  0.20 ( 0.19 /  0.20)\tMoving average: 0.123\n",
      "Episode 1100\tLast score:  0.09 ( 0.09 /  0.00)\tMoving average: 0.140\n",
      "Episode 1200\tLast score:  0.10 ( 0.10 /  0.09)\tMoving average: 0.124\n",
      "Episode 1300\tLast score:  0.10 ( 0.10 /  0.09)\tMoving average: 0.116\n",
      "Episode 1400\tLast score:  0.10 ( 0.09 /  0.10)\tMoving average: 0.113\n",
      "Episode 1487\tLast score:  0.19 ( 0.10 /  0.19)\tMoving average: 0.114"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the agent\n",
    "print('Starting training phase using these settings:')\n",
    "print(\"\\n '\".join(str(config.__dict__).split(\", '\")), '\\n')\n",
    "\n",
    "scores, agents_scores, moving_avg = train(env, maddpg, config.max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## Plot graphic of rewards\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Trace a line indicating the target value\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m target \u001b[38;5;241m=\u001b[39m [config\u001b[38;5;241m.\u001b[39mtarget_score] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mscores\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Graphic with the total rewards\u001b[39;00m\n\u001b[1;32m     10\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "## Plot graphic of rewards\n",
    "\n",
    "# Trace a line indicating the target value\n",
    "target = [config.target_score] * len(scores)\n",
    "\n",
    "# Graphic with the total rewards\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Plot of the rewards', fontsize='xx-large')\n",
    "ax.plot(scores, label='Score', color='Blue')\n",
    "ax.plot(moving_avg, label='Moving Average',\n",
    "        color='DarkOrange', linewidth=3)\n",
    "ax.plot(target, linestyle='--', color='LightCoral', linewidth=1 )\n",
    "ax.text(0, config.target_score, 'Target', color='LightCoral', fontsize='large')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Episode #')\n",
    "ax.legend(fontsize='xx-large')\n",
    "plt.show()\n",
    "\n",
    "# Graphics for each one of the agents\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8), sharex=True, sharey=True)\n",
    "fig.suptitle('Rewards for each one of the agents', fontsize='xx-large')\n",
    "axs = axs.flatten()\n",
    "for idx, (ax, s) in enumerate(zip(axs, np.transpose(agents_scores))):\n",
    "    ax.plot(s, label='Agent Score', color='DodgerBlue', zorder=1)\n",
    "    ax.plot(moving_avg, label='Moving Avg (Total)',\n",
    "            color='DarkOrange', linewidth=3, alpha=0.655, zorder=2)\n",
    "    ax.plot(target, linestyle='--', color='LightCoral', linewidth=1, zorder=0)\n",
    "    ax.text(0, config.target_score, 'Target',\n",
    "            color='LightCoral', fontsize='large')\n",
    "    ax.set_title('Agent #%d' % (idx+1))\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Episode #')\n",
    "    ax.label_outer()\n",
    "    ax.legend(fontsize='medium')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'maddpg_state_dict': maddpg.state_dict(),\n",
    "    'scores': scores,\n",
    "    'agents_scores': agents_scores,\n",
    "    'moving_avg': moving_avg}\n",
    "torch.save(checkpoint, 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Implementation\n",
    "\n",
    "This cell implements the evaluation process for the trained MADDPG system.\n",
    "\n",
    "## Purpose\n",
    "The `test` function evaluates the performance of trained agents in the environment without exploration noise, providing a clear assessment of the learned policies.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Test Configuration\n",
    "- Disables training mode in environment\n",
    "- Turns off exploration noise\n",
    "- Loads pretrained model weights\n",
    "\n",
    "### Evaluation Process\n",
    "1. **Episode Setup**\n",
    "   - Reset environment\n",
    "   - Initialize score tracking\n",
    "\n",
    "2. **Episode Execution**\n",
    "   - Get agent observations\n",
    "   - Select deterministic actions\n",
    "   - Execute environment steps\n",
    "   - Track rewards\n",
    "\n",
    "3. **Performance Reporting**\n",
    "   - Maximum score across agents\n",
    "   - Individual agent scores\n",
    "   - Episode-by-episode results\n",
    "\n",
    "## Implementation Details\n",
    "- Uses saved checkpoint for model weights\n",
    "- Runs specified number of test episodes\n",
    "- Provides detailed performance metrics\n",
    "- Maintains original environment structure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env: UnityEnvironment, maddpg: MultiAgentDDPG, \n",
    "         max_episodes: int = 3) -> None:\n",
    "    \"\"\"Test a trained MADDPG system in the environment.\n",
    "    \n",
    "    This function runs the trained agents in evaluation mode (no exploration noise)\n",
    "    for a specified number of episodes and reports their performance.\n",
    "    \n",
    "    Args:\n",
    "        env: Unity environment for agent interaction\n",
    "        maddpg: Trained multi-agent DDPG system\n",
    "        max_episodes: Number of test episodes to run (default: 3)\n",
    "        \n",
    "    Note:\n",
    "        - Training mode is disabled during testing\n",
    "        - Scores are reported for each agent and the maximum score\n",
    "        - Episodes run until completion (local_done signal)\n",
    "    \"\"\"\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        # Reset environment for new episode\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        scores = np.zeros(config.num_agents)\n",
    "        \n",
    "        # Episode execution loop\n",
    "        while True:\n",
    "            # Get actions from agents and execute step\n",
    "            states = env_info.vector_observations\n",
    "            actions = maddpg.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            # Update scores and check for episode end\n",
    "            scores += env_info.rewards\n",
    "            states = env_info.vector_observations\n",
    "            if any(env_info.local_done):\n",
    "                break\n",
    "                \n",
    "        # Report episode results\n",
    "        print('\\rEpisode {:4d}\\tScore: {:5.2f} ({:5.2f} / {:5.2f})'\n",
    "              .format(i_episode, scores.max(), scores[0], scores[1]))\n",
    "\n",
    "\n",
    "# Load trained model and configure for testing\n",
    "checkpoint = torch.load('checkpoint.pt')\n",
    "config.noise = False  # Disable exploration noise for testing\n",
    "maddpg = MultiAgentDDPG()\n",
    "maddpg.load_state_dict(checkpoint['maddpg_state_dict'])\n",
    "\n",
    "# Run test episodes\n",
    "test(env, maddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (udacity_rl_new)",
   "language": "python",
   "name": "udacity_rl_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
